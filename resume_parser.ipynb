{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeAPoWqCFJwzd5c3CsFqe4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudamah/sentence-similarity/blob/main/resume_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Resume Parser** \n",
        "\n",
        "*   **Will Use Transformer and Spacy to Train and Predict the Model**\n",
        "*   **NER**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0IgkUyriLkwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy_transformers\n",
        "!pip install -U spacy\n",
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "id": "yP61aumtTF4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "6zD8cr3qxipv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "C6jo7jjN3J77"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFFPchHdxisw",
        "outputId": "fa115cee-e9f6-455c-a3ba-58afb3b6fb62"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr  3 16:49:06 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0    26W /  70W |    363MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_data = json.load(open('/content/resume_parser/data/training/train_data.json', 'r'))"
      ],
      "metadata": {
        "id": "ju9-dyuXUTRR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cv_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddflnR_IWG5B",
        "outputId": "27057ae3-8bf0-49dd-af91-2866220639be"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config /content/resume_parser/data/training/base_config.cfg /content/resume_parser/data/training/config.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE4MjqEKWG7d",
        "outputId": "a2ab0208-9937-46a1-985e-63955cab2bff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-03 16:49:30.410850: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-04-03 16:49:30.410955: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-04-03 16:49:30.410973: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "/content/resume_parser/data/training/config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_spacy_doc(data):\n",
        "    nlp = spacy.blank('en')\n",
        "    db = DocBin()\n",
        "\n",
        "    for text, annot in tqdm(data):\n",
        "        doc = nlp.make_doc(text)\n",
        "        annot = annot['entities']\n",
        "\n",
        "        ents = []\n",
        "        entity_indices = []\n",
        "\n",
        "        for start, end, label in annot:\n",
        "            skip_entity = False\n",
        "            for idx in range(start, end):\n",
        "                if idx in entity_indices:\n",
        "                    skip_entity = True\n",
        "                    break\n",
        "            if skip_entity == True:\n",
        "                continue\n",
        "\n",
        "            entity_indices = entity_indices + list(range(start, end))\n",
        "\n",
        "            try:\n",
        "                span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
        "            except:\n",
        "                continue\n",
        "            \n",
        "            # if span is None:\n",
        "            #     err_data = str([start, end]) + \"   \" + str(text) + '\\n'\n",
        "            #     file.write(err_data)\n",
        "\n",
        "            if span is not None:\n",
        "                ents.append(span)\n",
        "        \n",
        "        try:\n",
        "            doc.ents = ents\n",
        "            db.add(doc)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return db\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R26Fz6G-WG9_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(cv_data, test_size=0.30)"
      ],
      "metadata": {
        "id": "U9lVnHGWWHAr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train), len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDiRcmD1WHDG",
        "outputId": "a8b94a48-97bd-43da-9d89-896baffd684f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(140, 60)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file = open('error.txt', 'r')"
      ],
      "metadata": {
        "id": "7HvCo38nWHFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = get_spacy_doc(train)\n",
        "db.to_disk('train_data.spacy')\n",
        "\n",
        "db = get_spacy_doc(test)\n",
        "db.to_disk('test_data.spacy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jowQ_pPSWHIu",
        "outputId": "7c29a97d-0492-400e-9ec1-58c920668987"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 140/140 [00:02<00:00, 65.92it/s]\n",
            "100%|██████████| 60/60 [00:00<00:00, 63.54it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(db.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL9VmKRTWHKo",
        "outputId": "2232bcd3-f9a5-45c4-fa04-91ac55160b8e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train /content/resume_parser/data/training/config.cfg --output ./output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCDoyxuWWHNX",
        "outputId": "affa9aa1-f095-4af5-8948-3bfd3ca7385d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-03 17:06:31.173265: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-04-03 17:06:31.173401: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-04-03 17:06:31.173423: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2023-04-03 17:06:37,092] [INFO] Set up nlp object from config\n",
            "[2023-04-03 17:06:37,102] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2023-04-03 17:06:37,106] [INFO] Created vocabulary\n",
            "[2023-04-03 17:06:37,107] [INFO] Finished initializing nlp object\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2023-04-03 17:06:44,764] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        2923.28   1593.73    0.10    0.05    1.97    0.00\n",
            "  3     200      137929.23  63182.17   41.18   51.38   34.36    0.41\n",
            "  7     400       19077.25  22439.14   55.69   55.58   55.79    0.56\n",
            " 11     600        7506.42  19450.81   48.81   39.21   64.66    0.49\n",
            " 15     800        3165.08  17693.37   55.91   74.95   44.58    0.56\n",
            " 19    1000      102254.26  18059.91   56.83   53.41   60.71    0.57\n",
            " 23    1200        1578.17  15422.01   55.68   64.66   48.89    0.56\n",
            " 26    1400       20562.63  14545.84   56.74   56.46   57.02    0.57\n",
            " 30    1600         651.96  13480.68   56.44   55.40   57.51    0.56\n",
            " 34    1800        3526.44  13381.35   56.02   65.35   49.01    0.56\n",
            " 38    2000       11261.56  12935.09   55.97   64.17   49.63    0.56\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TkRuH_9NDYX5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Test**"
      ],
      "metadata": {
        "id": "T7WSQcfiGdz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('/content/output/model-best')"
      ],
      "metadata": {
        "id": "iXSvUvkzDYad"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/output.zip /content/output"
      ],
      "metadata": {
        "id": "4qCZPMkOO2kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/output.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "UdYb2cqnPj03",
        "outputId": "728eaaae-91e2-4f42-edf1-a152af80b4f7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_23301c1d-bc3f-42b5-ab1c-cb8bf8315576\", \"output.zip\", 872265429)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C_VxT1N8Q1_t"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, fitz"
      ],
      "metadata": {
        "id": "4_xLSyfDHRrc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname = '/content/resume_parser/data/test/Alice Clark CV.pdf'\n",
        "doc = fitz.open(fname)"
      ],
      "metadata": {
        "id": "qjToD9MzDYiu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" \"\n",
        "for page in doc:\n",
        "    text = text + str(page.get_text())"
      ],
      "metadata": {
        "id": "UODrW9ekUTVM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "wPCHNqpRNA6g",
        "outputId": "714bdfe9-298f-40b9-c149-fc55f8181260"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Alice Clark \\nAI / Machine Learning \\n \\nDelhi, India Email me on Indeed \\n• \\n20+ years of experience in data handling, design, and development \\n• \\nData Warehouse: Data analysis, star/snow flake scema data modelling and design specific to \\ndata warehousing and business intelligence \\n• \\nDatabase: Experience in database designing, scalability, back-up and recovery, writing and \\noptimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes. \\nCloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure, \\nStream Analytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake \\nanalytics(U-SQL) \\nWilling to relocate anywhere \\n \\nWORK EXPERIENCE \\nSoftware Engineer \\nMicrosoft – Bangalore, Karnataka \\nJanuary 2000 to Present \\n1. Microsoft Rewards Live dashboards: \\nDescription: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping \\nonline. Microsoft Rewards members can earn points when searching with Bing, browsing with \\nMicrosoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft \\nStore. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft \\nrewards website. Rewards live dashboards gives a live picture of usage world-wide and by \\nmarkets like US, Canada, Australia, new user registration count, top/bottom performing rewards \\noffers, orders stats and weekly trends of user activities, orders and new user registrations. the \\nPBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes. \\nTechnology/Tools used \\n \\nEDUCATION \\nIndian Institute of Technology – Mumbai \\n2001 \\n \\nSKILLS \\nMachine Learning, Natural Language Processing, and Big Data Handling \\n \\nADDITIONAL INFORMATION \\nProfessional Skills \\n• Excellent analytical, problem solving, communication, knowledge transfer and interpersonal \\nskills with ability to interact with individuals at all the levels \\n• Quick learner and maintains cordial relationship with project manager and team members and \\ngood performer both in team and independent job environments \\n• Positive attitude towards superiors &amp; peers \\n• Supervised junior developers throughout project lifecycle and provided technical assistance \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" \".join(text.split())"
      ],
      "metadata": {
        "id": "Y3trvmz9yBtW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1-R6PYxqM9u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)\n",
        "for ent in  doc.ents:\n",
        "    print(ent.label_,  '----------', ent.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lSS3AFmxok4",
        "outputId": "8fc1c741-069a-4d0b-e91f-41d48234ed98"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name ---------- Alice Clark\n",
            "Degree ---------- AI / Machine Learning\n",
            "Location ---------- Delhi\n",
            "Years of Experience ---------- 20+ years\n",
            "Companies worked at ---------- Microsoft\n",
            "Designation ---------- Software Engineer\n",
            "Companies worked at ---------- Microsoft\n",
            "Location ---------- Bangalore\n",
            "Companies worked at ---------- Microsoft\n",
            "Companies worked at ---------- Microsoft\n",
            "Companies worked at ---------- Microsoft\n",
            "Companies worked at ---------- Microsoft\n",
            "Companies worked at ---------- Microsoft\n",
            "Companies worked at ---------- Microsoft\n",
            "College Name ---------- Indian Institute of Technology\n",
            "Location ---------- Mumbai\n",
            "Skills ---------- Machine Learning, Natural Language Processing, and Big Data Handling ADDITIONAL INFORMATION Professional Skills • Excellent analytical, problem solving, communication, knowledge transfer and interpersonal skills with ability to interact with individuals at all the levels • Quick learner and maintains cordial relationship with project manager and team members and good performer both in team and independent job environments • Positive attitude towards superiors &amp; peers • Supervised junior developers throughout project lifecycle and provided technical assistance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M40xlsjhNHpF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}